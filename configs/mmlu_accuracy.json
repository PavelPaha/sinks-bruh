{
  "task": "mmlu_accuracy_sink",
  "defaults": {
    "samples": 2000,
    "sink_tokens": 4,
    "chat": "auto",
    "quantization": "none",
    "device": "cuda"
  },
  "models": [
    "mistralai/Mistral-7B-v0.1",
    "mistralai/Mistral-Nemo-Instruct-2407",
    "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "Qwen/Qwen2.5-7B-Instruct",
    "Qwen/Qwen2.5-14B-Instruct",
    "Qwen/Qwen2.5-72B-Instruct"
  ]
}

